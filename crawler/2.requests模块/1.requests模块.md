## 摘要
  主要学习requests这个http模块，该模块主要用于发送请求获取响应，该模块有很多的替代模块，比如说urllib模块，
但是在工作中用的最多的还是requests模块，requests的代码简洁易懂，相对于臃肿的urllib模块，使用requests编写的爬虫代码将会更少，
而且实现某一功能将会简单。因此建议大家掌握该模块的使用。  

知识点：
* 掌握 headers参数的使用
* 掌握 发送带参数的请求
* 掌握 headers中携带cookie
* 掌握 cookies参数的使用
* 掌握 cookieJar的转换方法
* 掌握 超时参数timeout的使用
* 掌握 代理ip参数proxies的使用
* 掌握 使用verify参数忽略CA证书
* 掌握 requests模块发送post请求
* 掌握 利用requests.session进行状态保持

## 内容  

### 1. requests模块介绍
```renderscript
requests文档: http://docs.python-requests.org/zh_CN/latest/index.html

* requests文档:https://requests.readthedocs.io/en/latest/
```

  其中的快速上手跟高级用法需要仔细阅读一遍。爬虫开发中，我们用到的主要是快速上手。高阶用法用的比较少一些。  
  
  

#### 1.1 requests模块的作用
* 发送http请求，获取响应数据。  

#### 1.2 requests模块是一个第三方模块，需要在你的python(虚拟)环境中额外安装

```renderscript
pip/pip3 install requests
```

#### 1.3 requests模块发送get请求
```renderscript
需求：通过requests向百度首页发送请求，获取该页面的源码
运行下面的代码，观察打印输出的结果
```

代码块如下：

```renderscript
# 1.2.1-简单的代码实现
import requests 
# 目标url
url = 'https://www.baidu.com' 
# 向目标url发送get请求
response = requests.get(url)
# 打印响应内容
print(response.text)
```

### 2. response响应对象

```renderscript
* 观察上边代码运行结果发现，有好多乱码；这是因为编解码使用的字符集不同早造成的；我们尝试使用下边的办法来解决中文乱码问题
```

代码如下:
```renderscript
# 1.2.2-response.content
import requests 
# 目标url
url = 'https://www.baidu.com' 
# 向目标url发送get请求
response = requests.get(url)
# 打印响应内容
# print(response.text)
print(response.content.decode()) # 注意这里！
```  

![image](../images/13.png)   

然后我们在响应的时候设置下其对应的编码：
```renderscript
import requests

url = 'https://www.baidu.com'
response = requests.get(url)

print(response.encoding)
response.encoding = 'utf8'
print(response.text)
print(response.encoding)
```

![image](../images/14.png)   

我们设置其响应编码之后，我们发现对应的响应结果为正常显示了。  

1. response.text是requests模块按照chardet模块推测出的编码字符集进行解码的结果
2. 网络传输的字符串都是bytes类型的，所以response.text = response.content.decode('推测出的编码字符集')
3. 我们可以在网页源码中搜索charset，尝试参考该编码字符集，注意存在不准确的情况


#### 2.1 response.text 和response.content的区别
* response.text
  * 类型：str
  * 解码类型： requests模块自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码

* response.content
  * 类型：bytes
  * 解码类型： 没有指定

#### 2.2 通过对response.content进行decode，来解决中文乱码
* response.content.decode() 默认utf-8
* response.content.decode("GBK")
* 常见的编码字符集
  * utf-8
  * gbk
  * gb2312
  * ascii （读音：阿斯克码）
  * iso-8859-1
  
#### 2.3 response响应对象的其它常用属性或方法
```renderscript
response = requests.get(url)中response是发送请求获取的响应对象；
response响应对象中除了text、content获取响应内容以外还有其它常用的属性或方法：
```  

* response.url响应的url；有时候响应的url和请求的url并不一致
* response.status_code 响应状态码
* response.request.headers 响应对应的请求头
* response.headers 响应头
* response.request._cookies 响应对应请求的cookie；返回cookieJar类型
* response.cookies 响应的cookie（经过了set-cookie动作；返回cookieJar类型
* response.json()自动将json字符串类型的响应内容转换为python对象（dict or list）

知识点：掌握 response响应对象的其它常用属性。

### 3. requests模块发送请求
#### 3.1 发送带header的请求
###### 爬虫获取数据大小
  首先，我们使用如下代码:
  
```renderscript
import requests

url = 'https://www.baidu.com'
response = requests.get(url)

# 第一种模式
print(len(response.content.decode()))
```

程序输出的结果为:2349  

```renderscript
<!DOCTYPE html>
<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class="bg s_ipt_wr"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus=autofocus></span><span class="bg s_btn_wr"><input type=submit id=su value=百度一下 class="bg s_btn" autofocus></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=https://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write('<a href="http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&")+ "bdorz_come=1")+ '" name="tj_login" class="lb">登录</a>');
                </script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>

```

###### 网页获取数据大小  
![image](../images/15.png)  
  发现大小为大于5000;发现其响应的结果除了上面的代码之外，还出现了css等样式。
这里面的区别到底在哪里？你不是模拟的浏览器吗？在发送请求的时候，我们可以预先设置一些请求头模拟浏览器。我们可以看到我们的浏览器请求baidu的时候带上
我们之前说的三大剑客：User-Agent、Referer、Cookie即可。  
由于目前我们访问baidu的时候是不需要用户保持的，所以是不需要使用cookie的。并且我们也没有从哪里来的情况，所以我们只需要
使用:User-Agent。

```renderscript
import requests
url = 'https://www.baidu.com'
# 构建请求头字典
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36"
}

# 发送请求
res = requests.get(url, headers=headers)
print(res.content.decode())
print(len(res.content.decode()))
```

上面输出的结果就是:365023;
上面就是告诉浏览器，我们使用了浏览器的代理了。  

##### 3.1.1 思考
![image](../images/16.png)  

##### 3.1.2 携带请求头发送请求的方法
```renderscript
requests.get(url, headers=headers)
```

* headers参数接收字典形式的请求头
* 请求头字段名作为key，字段对应的值作为value

#### 3.2 发送带参数的请求
```renderscript
我们在使用百度搜索的时候经常发现url地址中会有一个 ?，那么该问号后边的就是请求参数，又叫做查询字符串.
```
##### 3.2.1 在url携带参数
  直接对含有参数的url发起请求。
  
```renderscript
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36"
}

url = "https://www.baidu.com?wd=python"

res = requests.get(url, headers=headers)
print(res.content.decode())
print(len(res.content.decode()))  # 365085
```

##### 3.2.2 通过params携带参数字典
​1. 构建请求参数字典  
​2. 向接口发送请求的时候带上参数字典，参数字典设置给params  

```renderscript
import requests
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36"
}
data = {
    "wd": "python"
}

# 带上请求参数发起请求，获取响应
res2 = requests.get(url, headers=headers, params=data)
print(res2.url)
```

#### 3.3 在headers参数中携带cookie

```renderscript
网站经常利用请求头中的Cookie字段来做用户访问状态的保持，那么我们可以在headers参数中添加Cookie，模拟普通用户的请求。
我们以github登陆为例：
```

##### 3.3.1 github登陆抓包分析
1. 打开浏览器，右键-检查，点击Net work，勾选Preserve log。
2. 访问github登陆的url地址 https://github.com/login。
3. 输入账号密码点击登陆后，访问一个需要登陆后才能获取正确内容的url，比如点击右上角的Your profile访问https://github.com/USER_NAME。
4. 确定url之后，再确定发送该请求所需要的请求头信息中的User-Agent和Cookie。

![image](../images/17.png)   
然后我们查看登录后的cookie如下：
![image](../images/18.png)   

###### 如何判断我们的账号是否已经登录成功呢？
  如下图是登录成功的图：
  ![image](../images/17.png)  
  无痕上网模式下(不携带cookie)，我们会出现其对应的Edit profile丢失。 
  ![image](../images/19.png)   
  我们可以知道有两个地方不一致：1、Edit profile   2、文章的title不一致；无痕上网是:startshineye (startshineye) · GitHub  

##### 3.3.2 代码实现
不携带cookie:

```renderscript
import requests
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36"
}

url = "https://github.com/startshineye"

res = requests.get(url, headers=headers)

with open("headers_without_cookie.html", "wb") as f:
    f.write(res.content)
```

![image](../images/20.png) 
携带cookie如下：

```renderscript
import requests
headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36",
    "cookie": "_ga=GA1.2.150843220.1593087436; _octo=GH1.1.1661912861.1656428524; _device_id=8655e7a2207006161e4c236120b39e84; color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; preferred_color_mode=light; tz=Asia%2FShanghai; user_session=4hLXjFwJvyMeNKw09K1g-tQ1TKUL5IXxjAVw-EOhi9QVyh0U; __Host-user_session_same_site=4hLXjFwJvyMeNKw09K1g-tQ1TKUL5IXxjAVw-EOhi9QVyh0U; tz=Asia%2FShanghai; logged_in=yes; dotcom_user=startshineye; has_recent_activity=1; _gh_sess=GP3Xf%2BGsjP0hFpI%2B5FhW%2BpWu28%2BvIOgqvXOMAPgHx8NpEc%2FRMVlobIu%2FwZ3GzD7vNPXB67vWkckQll9ers3jGu2OmJlR2qtUVRhsuQmzPRF7dF0QQf2Pa0Q%2FO4HcRwg2yY06M3G%2B%2FM2CyI%2FY8kJWAjHqpi0PkouTmwl1aWgSI693iKYhG9%2FM4iWT3pNnVLA2wRbnt6HVwmOPKqdmWazmQTqQDc5jBQNoptemv2UbimWLG%2FWorAgIpQ0%2FELIoZKmcWCFd07vbeKzScI1Tkz9TcHILreg%2BDG2mx%2BrOwEqqZNV4glHvaRL%2Bes4u%2BJLzi4o8UDDL5u1UPg07rgcgcCpPYIQlA%2FkS4dLD7sSFPGeGHvmOx4t8sm9pz%2BOWE6JBhL%2FJ7o3fAEcXOrl9QQ%2F1IIsGY2yyt%2FwXSVnN7HvE98SOCPKvDGNZK2tyNSov077VD%2BhUPUGVKbbYBeA3BELUeeqtl7U3QNU%2BUP1wctnwb9Q7ao6nNit289eLfahTExuz%2FwyLwoPmmMNVCNpc0GWrxyKE6p%2B8laPS63gLdtxZTPs5pHV6Zya%2B0r3emhwcNlqFGTkUb5xkSNC2ueYNILutuaGHI7tkZ0aLy1ykrlUkk76DowdmvEMlMHzHn5%2FAod3Van09pxJ1jwjrL9qF2q%2BFmV8DFd8EpjGksglQH%2BHWrjaqkWRhDe9zq99DOoRPO4PbzUwPMVysSN0I9dM%2B2KTQI1ihwGMbeMwnUIEAHKm1bbMn9VXA5%2BPfcP%2BB%2BD9lxgWU0xSpEgw12NMSQTN3MyGI9Bf5K12iTo2iXQLs1Rp%2FZKrNnFvJwEgBwkz8SMzPrqroFdrbIsL%2BjA6zXEKc8tuQnVFrUXWmm5UyWebmoRm04SI%2B4dYIl0VUBYIZCLIkGYNGKfy6alvIz20qfmswKbnIEBNX46OLXFr0BXnA8BDgwqU%2BjvTjtHnUVa1x7doFWtJD2W15p4hXfBprUfHwUuRNkx4arl0zG%2BKcH4Eur085Y%2BMP6zcEWJ%2FbecYTAwezS7Wb0jOlvZVYmsmIyJ6LQXl8kB3yhgm5%2FJHQHtGwnN09Yo7W4zyijR7dAlG5wAU%2B%2BJZWfMIFodIUEukNXF1nRXHJ2u8TXrOvhua14PIJ6cQCZwTRWpFwbY9z--To4%2Bn6fmogIVXjJV--Ju9kJHbmu11GsQ7mkE2gnw%3D%3D"
}
url = "https://github.com/startshineye"
res = requests.get(url, headers=headers)
with open("headers_with_cookie.html", "wb") as f:
    f.write(res.content)
```

##### 3.3.3 运行代码验证结果

```renderscript
在打印的输出结果中搜索title，html中的标题文本内容如果是你的github账号，则成功利用headers参数携带cookie，
获取登陆后才能访问的页面
```
  
#### 3.4 cookies参数的使用
```renderscript
headers参数中携带cookie，也可以使用专门的cookies参数
```  
1. cookies参数的形式：字典
  cookies = {"cookie的name":"cookie的value"}

  * 该字典对应请求头中Cookie字符串，以分号、空格分割每一对字典键值对
  * 等号左边的是一个cookie的name，对应cookies字典的key
  * 等号右边对应cookies字典的value
  
2. cookies参数的使用方法
  response = requests.get(url, cookies)

3. 将cookie字符串转换为cookies参数所需的字典：
cookies_dict = {cookie.split('=')[0]:cookie.split('=')[-1] for cookie in cookies_str.split('; ')}

4. 注意：cookie一般是有过期时间的，一旦过期需要重新获取

代码如下:

```renderscript
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36",
}

url = "https://github.com/startshineye"

# cookie_str
cookies_str = "_ga=GA1.2.150843220.1593087436; _octo=GH1.1.1661912861.1656428524; _device_id=8655e7a2207006161e4c236120b39e84; color_mode=%7B%22color_mode%22%3A%22auto%22%2C%22light_theme%22%3A%7B%22name%22%3A%22light%22%2C%22color_mode%22%3A%22light%22%7D%2C%22dark_theme%22%3A%7B%22name%22%3A%22dark%22%2C%22color_mode%22%3A%22dark%22%7D%7D; preferred_color_mode=light; tz=Asia%2FShanghai; user_session=4hLXjFwJvyMeNKw09K1g-tQ1TKUL5IXxjAVw-EOhi9QVyh0U; __Host-user_session_same_site=4hLXjFwJvyMeNKw09K1g-tQ1TKUL5IXxjAVw-EOhi9QVyh0U; tz=Asia%2FShanghai; logged_in=yes; dotcom_user=startshineye; has_recent_activity=1; _gh_sess=GP3Xf%2BGsjP0hFpI%2B5FhW%2BpWu28%2BvIOgqvXOMAPgHx8NpEc%2FRMVlobIu%2FwZ3GzD7vNPXB67vWkckQll9ers3jGu2OmJlR2qtUVRhsuQmzPRF7dF0QQf2Pa0Q%2FO4HcRwg2yY06M3G%2B%2FM2CyI%2FY8kJWAjHqpi0PkouTmwl1aWgSI693iKYhG9%2FM4iWT3pNnVLA2wRbnt6HVwmOPKqdmWazmQTqQDc5jBQNoptemv2UbimWLG%2FWorAgIpQ0%2FELIoZKmcWCFd07vbeKzScI1Tkz9TcHILreg%2BDG2mx%2BrOwEqqZNV4glHvaRL%2Bes4u%2BJLzi4o8UDDL5u1UPg07rgcgcCpPYIQlA%2FkS4dLD7sSFPGeGHvmOx4t8sm9pz%2BOWE6JBhL%2FJ7o3fAEcXOrl9QQ%2F1IIsGY2yyt%2FwXSVnN7HvE98SOCPKvDGNZK2tyNSov077VD%2BhUPUGVKbbYBeA3BELUeeqtl7U3QNU%2BUP1wctnwb9Q7ao6nNit289eLfahTExuz%2FwyLwoPmmMNVCNpc0GWrxyKE6p%2B8laPS63gLdtxZTPs5pHV6Zya%2B0r3emhwcNlqFGTkUb5xkSNC2ueYNILutuaGHI7tkZ0aLy1ykrlUkk76DowdmvEMlMHzHn5%2FAod3Van09pxJ1jwjrL9qF2q%2BFmV8DFd8EpjGksglQH%2BHWrjaqkWRhDe9zq99DOoRPO4PbzUwPMVysSN0I9dM%2B2KTQI1ihwGMbeMwnUIEAHKm1bbMn9VXA5%2BPfcP%2BB%2BD9lxgWU0xSpEgw12NMSQTN3MyGI9Bf5K12iTo2iXQLs1Rp%2FZKrNnFvJwEgBwkz8SMzPrqroFdrbIsL%2BjA6zXEKc8tuQnVFrUXWmm5UyWebmoRm04SI%2B4dYIl0VUBYIZCLIkGYNGKfy6alvIz20qfmswKbnIEBNX46OLXFr0BXnA8BDgwqU%2BjvTjtHnUVa1x7doFWtJD2W15p4hXfBprUfHwUuRNkx4arl0zG%2BKcH4Eur085Y%2BMP6zcEWJ%2FbecYTAwezS7Wb0jOlvZVYmsmIyJ6LQXl8kB3yhgm5%2FJHQHtGwnN09Yo7W4zyijR7dAlG5wAU%2B%2BJZWfMIFodIUEukNXF1nRXHJ2u8TXrOvhua14PIJ6cQCZwTRWpFwbY9z--To4%2Bn6fmogIVXjJV--Ju9kJHbmu11GsQ7mkE2gnw%3D%3D"

# 构造cookies字典
cookies_dict = {}

cookies_list = cookies_str.split('; ')

cookies_dict = {cookie.split("=")[0]: cookie.split("=")[-1] for cookie in cookies_list}

'''
for cookie in cookies_list:
    # _ga=GA1.2.150843220.1593087436
    cookies_dict[cookie.split("=")[0]] = cookie.split("=")[-1]
'''

print(cookies_dict)
res = requests.get(url, headers=headers, cookies=cookies_dict)

with open("cookies.html", "wb") as f:
    f.write(res.content)
```

#### 3.5 cookieJar对象转换为cookies字典的方法
```renderscript
使用requests获取的resposne对象，具有cookies属性。该属性值是一个cookieJar类型，包含了对方服务器设置在本地的cookie。我们如何将其转换为cookies字典呢？
```

1. 转换方法
  cookies_dict = requests.utils.dict_from_cookiejar(response.cookies)
2. 其中response.cookies返回的就是cookieJar类型的对象。
3. requests.utils.dict_from_cookiejar函数返回cookies字典。

代码:

```renderscript
import requests
import requests.utils

url = "https://www.baidu.com"
res = requests.get(url)

# cookieJar对象
print(res.cookies)

# cookieJar对象转换成cookies对象
cookies_dict = requests.utils.dict_from_cookiejar(res.cookies)
print(cookies_dict)

# cookies对象转换成cookieJar
cookieJar = requests.utils.cookiejar_from_dict(cookies_dict)
print(cookieJar)
```

输出结果:

```renderscript
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
{'BDORZ': '27315'}
<RequestsCookieJar[<Cookie BDORZ=27315 for />]>
```

#### 3.6 超时参数timeout的使用
```renderscript
1. 在平时网上冲浪的过程中，我们经常会遇到网络波动，这个时候，一个请求等了很久可能任然没有结果。
2. 在爬虫中，一个请求很久没有结果，就会让整个项目的效率变得非常低，这个时候我们就需要对请求进行强制要求，让他必须在特定的时间内返回结果，否则就报错。
```

1. 超时参数timeout的使用方法
   * response = requests.get(url, timeout=3)
2. timeout=3表示：发送请求后，3秒钟内返回响应，否则就抛出异常

代码如下:  

```renderscript
import requests
url = "https://twitter.com"
res = requests.get(url, timeout=3)
```


  
  
  



