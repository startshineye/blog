## 常见的反爬手段和解决思路
内容:
1. 了解服务器反爬的原因
2. 了解服务器常反什么样的爬虫
3. 了解反爬虫领域常见的一些概念
4. 了解反爬的三个方向
5. 了解常见基于身份识别进行反爬
6. 了解常见基于爬虫行为进行反爬
7. 了解常见基于数据加密进行反爬

### 1、服务器反爬的原因
   主要是以下3点原因：
1. 爬虫占总PV(PV是指页面的访问次数，每打开或刷新一次页面，就算做一个pv)比例较高，这样浪费资源(钱)（尤其是三月份爬虫）。
   三月份爬虫是个什么概念呢？每年的三月份我们会迎接一次爬虫高峰期，有大量的硕士在写论文的时候会选择爬取一些往网站，并进行舆情分析。因为五月份交论文，所以嘛，大家都是读过书的，你们懂的，前期各种DotA，LOL，到了三月份了，来不及了，赶紧抓数据，四月份分析一下，五月份交论文，就是这么个节奏。
2. 公司可免费查询的资源被批量抓走，丧失竞争力，这样少赚钱。
   数据可以在非登录状态下直接被查询。如果强制登陆，那么可以通过封杀账号的方式让对方付出代价，这也是很多网站的做法。但是不强制对方登录。那么如果没有反爬虫，对方就可以批量复制的信息，公司竞争力就会大大减少。竞争对手可以抓到数据，时间长了用户就会知道，只需要去竞争对手那里就可以了，没必要来我们网站，这对我们是不利的。
3. 法律的灰色地带: 状告爬虫成功的几率小
   爬虫在国内还是个擦边球，就是有可能可以起诉成功，也可能完全无效。所以还是需要用技术手段来做最后的保障。
   
### 2、服务器常反什么样的爬虫
  主要分为以下5类:
1. 十分低级的应届毕业生
   应届毕业生的爬虫通常简单粗暴，根本不管服务器压力，加上人数不可预测，很容易把站点弄挂。

2. 十分低级的创业小公司
   现在的创业公司越来越多，也不知道是被谁忽悠的然后大家创业了发现不知道干什么好，觉得大数据比较热，就开始做大数据。分析程序全写差不多了，发现自己手头没有数据。怎么办？写爬虫爬啊。于是就有了不计其数的小爬虫，出于公司生死存亡的考虑，不断爬取数据。

3. 不小心写错了没人去停止的失控小爬虫
   有些网站已经做了相应的反爬，但是爬虫依然孜孜不倦地爬取。什么意思呢？就是说，他们根本爬不到任何数据，除了httpcode是200以外，一切都是不对的，可是爬虫依然不停止这个很可能就是一些托管在某些服务器上的小爬虫，已经无人认领了，依然在辛勤地工作着。

4. 成型的商业对手
   爬取网站的这个公司，这个公司这个是最大的对手，他们有技术，有钱，要什么有什么，如果和你死磕，你就只能硬着头皮和他死磕，你需要进行反爬。

5. 抽风的搜索引擎
   大家不要以为搜索引擎都是好人，他们也有抽风的时候，而且一抽风就会导致服务器性能下降，请求量跟网络攻击没什么区别。

### 3、反爬虫领域常见的一些概念
因为反爬虫暂时是个较新的领域，因此有些定义要自己下：
* 爬虫：使用任何技术手段，批量获取网站信息的一种方式。关键在于批量。

* 反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。

* 误伤：在反爬虫的过程中，服务器端错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。

* 拦截：成功地阻止爬虫访问。这里会有拦截率的概念。通常来说，拦截率越高的反爬虫策略，误伤的可能性就越高。因此需要做个权衡。

* 资源：在反爬的时候，投入机器成本与人力成本的总和。


总结:
  这里要切记，人力成本也是资源，而且比机器更重要。因为，根据摩尔定律，机器越来越便宜。而根据IT行业的发展趋势，程序员工资越来越贵。因此，通常服务器反爬就是让爬虫工程师加班才是王道，机器成本并不是特别值钱。

### 4、反爬的三个方向
* 基于身份识别进行反爬
  通常根据发送过来的请求是否符合浏览器的请求，比如没有携带浏览器常见的参数就认为不正确。
* 基于爬虫行为进行反爬
  我们常见的爬虫都是有一个套路就是：先到一个列表页面，然后在列表页面先获取到一个列的url,然后再获取详情。我们可以根据这个套路去预断这种行为，然后指定策略。
* 基于数据加密进行反爬
  在我们爬取过程中，解析出来的数据可能是一些乱码数据，无形中增加了我们数据提取的难度。
  
  通常来说就是这3个方向。
